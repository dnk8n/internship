{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b00ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Tuple, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.data import (\n",
    "    DataLoader,\n",
    "    DatasetReader,\n",
    "    Instance,\n",
    "    TextFieldTensors,\n",
    "    Vocabulary,\n",
    ")\n",
    "from allennlp.data.data_loaders import SimpleDataLoader\n",
    "from allennlp.data.fields import Field, LabelField, TextField\n",
    "from allennlp.data.token_indexers import (\n",
    "    PretrainedTransformerIndexer,\n",
    "    SingleIdTokenIndexer,\n",
    "    TokenIndexer,\n",
    ")\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer\n",
    "from allennlp.data.tokenizers.pretrained_transformer_tokenizer import (\n",
    "    PretrainedTransformerTokenizer,\n",
    ")\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules import Seq2VecEncoder, TextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder, BertPooler\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.pretrained_transformer_embedder import (\n",
    "    PretrainedTransformerEmbedder,\n",
    ")\n",
    "from allennlp.nn import util\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp.training.gradient_descent_trainer import GradientDescentTrainer\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.training.optimizers import HuggingfaceAdamWOptimizer\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.training.util import evaluate\n",
    "\n",
    "\n",
    "# There's a warning when you call `forward_on_instances` that you don't need\n",
    "# to worry about right now, so we silence it.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PandasDataFrameType = TypeVar('pandas.core.frame.DataFrame')\n",
    "\n",
    "input_data_path = Path(\"../../../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fda5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(input_data_path / \"input.xlsx\")\n",
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_amazon = df['texts'].str.contains(\"amazon\", case=False, regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_amazon = df[with_amazon]\n",
    "df_with_amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c003e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_amazon = df[~with_amazon]\n",
    "df_without_amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba940e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus_on_keyword(text, keyword, size=5):\n",
    "    # Centres focus on keyword\n",
    "    output_text = \"\"\n",
    "    for i, j in enumerate(text.split()):\n",
    "        if keyword.lower() in j.lower():\n",
    "            output_text += \" \".join(text.split()[max([0, i - size]): max([size * 2 + 1, i + size + 1])]) + \"\\n\"\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_amazon['snippets'] = df_with_amazon.apply(lambda row: focus_on_keyword(row.texts, \"amazon\"), axis='columns')\n",
    "df_with_amazon.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_validate_labels = pd.read_excel(input_data_path / \"train-validate-labels.xlsx\", index_col=0)\n",
    "df_test_labels = pd.read_excel(input_data_path / \"test-labels.xlsx\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b282264",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {1: \"company\", 2: \"not company\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a2c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_validate = pd.merge(df_train_validate_labels, df_with_amazon, how='left', left_index=True, right_index=True)\n",
    "df_train_validate.insert(1, \"mapped_label\", df_train_validate[\"label\"].map(label_mapping))\n",
    "print(df_train_validate.shape)\n",
    "df_train_validate.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6dfc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.merge(df_test_labels, df_with_amazon, how='left', left_index=True, right_index=True)\n",
    "df_test.insert(1, \"mapped_label\", df_test[\"label\"].map(label_mapping))\n",
    "print(df_test.shape)\n",
    "df_test.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9342e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_indices = list(df_train_validate.index.values) + list(df_test.index.values)\n",
    "df_unseen = df_with_amazon.loc[~df_with_amazon.index.isin(seen_indices)]\n",
    "print(df_unseen.shape)\n",
    "df_unseen.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568aeb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_validate.sample(frac=0.8, random_state=42)\n",
    "print(df_train.shape)\n",
    "df_train.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validate = df_train_validate.loc[set(df_train_validate.index) - set(df_train.index)]\n",
    "print(df_validate.shape)\n",
    "df_validate.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81caad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@DatasetReader.register('classification-df')\n",
    "class ClassificationDfReader(DatasetReader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_header: str,\n",
    "        label_header: str,\n",
    "        tokenizer: Tokenizer = None,\n",
    "        token_indexers: Dict[str, TokenIndexer] = None,\n",
    "        max_tokens: int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.text_header = text_header\n",
    "        self.label_header = label_header\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def text_to_instance(self, text: str, label: str = None) -> Instance:        \n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        if self.max_tokens:\n",
    "            tokens = tokens[: self.max_tokens]\n",
    "        text_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {'text': text_field}\n",
    "        if label:\n",
    "            fields['label'] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "        \n",
    "    def _read(self, dataframe: PandasDataFrameType) -> Iterable[Instance]:\n",
    "        for row in dataframe.itertuples():\n",
    "            text = getattr(row, self.text_header)\n",
    "            label = getattr(row, self.label_header)\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            if self.max_tokens:\n",
    "                tokens = tokens[: self.max_tokens]\n",
    "            text_field = TextField(tokens, self.token_indexers)\n",
    "            label_field = LabelField(label)\n",
    "            fields: Dict[str, Field] = {\"text\": text_field, \"label\": label_field}\n",
    "            yield Instance(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(Model):\n",
    "    def __init__(\n",
    "        self, vocab: Vocabulary, embedder: TextFieldEmbedder, encoder: Seq2VecEncoder\n",
    "    ):\n",
    "        super().__init__(vocab)\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        num_labels = vocab.get_vocab_size(\"labels\")\n",
    "        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        text: TextFieldTensors,\n",
    "        label: torch.Tensor = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # Shape: (batch_size, num_tokens)\n",
    "        mask = util.get_text_field_mask(text)\n",
    "        # Shape: (batch_size, encoding_dim)\n",
    "        encoded_text = self.encoder(embedded_text, mask)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        logits = self.classifier(encoded_text)\n",
    "        # Shape: (batch_size, num_labels)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        output = {'probs': probs}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            # Shape: (1,)\n",
    "            output['loss'] = torch.nn.functional.cross_entropy(logits, label)\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5647643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_reader(*args, **kwargs) -> DatasetReader:\n",
    "    return ClassificationDfReader(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13daae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(instances: Iterable[Instance]) -> Vocabulary:\n",
    "    print(\"Building the vocabulary\")\n",
    "    return Vocabulary.from_instances(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54760bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab: Vocabulary) -> Model:\n",
    "    print(\"Building the model\")\n",
    "    vocab_size = vocab.get_vocab_size(\"tokens\")\n",
    "    embedder = BasicTextFieldEmbedder(\n",
    "        {\"bert\": PretrainedTransformerEmbedder(model_name=\"bert-base-uncased\")}\n",
    "    )\n",
    "    encoder = BertPooler(pretrained_model=\"bert-base-uncased\")\n",
    "    return SimpleClassifier(vocab, embedder, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loaders(\n",
    "    train_data: List[Instance],\n",
    "    dev_data: List[Instance],\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    train_loader = SimpleDataLoader(train_data, 8, shuffle=True)\n",
    "    dev_loader = SimpleDataLoader(dev_data, 8, shuffle=False)\n",
    "    return train_loader, dev_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trainer(\n",
    "    model: Model,\n",
    "    serialization_dir: str,\n",
    "    train_loader: DataLoader,\n",
    "    dev_loader: DataLoader,\n",
    ") -> Trainer:\n",
    "    parameters = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "    optimizer = HuggingfaceAdamWOptimizer(parameters)  # type: ignore\n",
    "    trainer = GradientDescentTrainer(\n",
    "        model=model,\n",
    "        serialization_dir=serialization_dir,\n",
    "        data_loader=train_loader,\n",
    "        validation_data_loader=dev_loader,\n",
    "        num_epochs=5,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(reader: DatasetReader) -> Tuple[List[Instance], List[Instance]]:\n",
    "    training_data = list(reader.read(df_train))\n",
    "    validation_data = list(reader.read(df_validate))\n",
    "    return training_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b45979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_loop(text_header, label_header, tokenizer, token_indexers, max_tokens):\n",
    "    dataset_reader = build_dataset_reader(text_header, label_header, tokenizer, token_indexers, max_tokens)\n",
    "\n",
    "    train_data, dev_data = read_data(dataset_reader)\n",
    "\n",
    "    vocab = build_vocab(train_data + dev_data)\n",
    "    model = build_model(vocab)\n",
    "\n",
    "    train_loader, dev_loader = build_data_loaders(train_data, dev_data)\n",
    "    train_loader.index_with(vocab)\n",
    "    dev_loader.index_with(vocab)\n",
    "\n",
    "    # You obviously won't want to create a temporary file for your training\n",
    "    # results, but for execution in binder for this guide, we need to do this.\n",
    "    with tempfile.TemporaryDirectory() as serialization_dir:\n",
    "        trainer = build_trainer(model, serialization_dir, train_loader, dev_loader)\n",
    "        print(\"Starting training\")\n",
    "        trainer.train()\n",
    "        print(\"Finished training\")\n",
    "\n",
    "    return model, dataset_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Predictor.register(\"context-classifier\")\n",
    "class ContextClassifierPredictor(Predictor):\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"sentence\": sentence})\n",
    "\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"sentence\"]\n",
    "        return self._dataset_reader.text_to_instance(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(predictor, vocab, dataframe):\n",
    "    for row in dataframe.itertuples():\n",
    "        output = predictor.predict(row.texts)\n",
    "        yield [row.snippets] + [\n",
    "            (vocab.get_token_from_index(label_id, \"labels\"), prob)\n",
    "            for label_id, prob in enumerate(output[\"probs\"])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, dataset_reader = run_training_loop(\n",
    "    text_header=\"texts\",\n",
    "    label_header=\"mapped_label\",\n",
    "    tokenizer=PretrainedTransformerTokenizer(model_name=\"bert-base-uncased\"),\n",
    "    token_indexers={\"bert\": PretrainedTransformerIndexer(model_name=\"bert-base-uncased\")},\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can evaluate the model on a new dataset.\n",
    "test_data = list(dataset_reader.read(df_test))\n",
    "data_loader = SimpleDataLoader(test_data, batch_size=8)\n",
    "data_loader.index_with(model.vocab)\n",
    "results = evaluate(model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2663565",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = model.vocab\n",
    "predictor = ContextClassifierPredictor(model, dataset_reader)\n",
    "# p = predictions(predictor, vocab, df_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139354a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_predictions(predictor, vocab, text):\n",
    "    output = predictor.predict(text)\n",
    "    index_max = np.argmax(output['probs'])\n",
    "    prediction = vocab.get_token_from_index(index_max, \"labels\")\n",
    "    return prediction, output['probs'][index_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156efd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen[['predicted_labels', 'confidence']] = df_unseen.apply(lambda row: apply_predictions(predictor, vocab, row.texts), axis='columns').apply(pd.Series)\n",
    "df_unseen.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unseen.to_excel(\"output.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
