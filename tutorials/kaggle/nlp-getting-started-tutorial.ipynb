{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Tutorial\n",
    "\n",
    "NLP - or *Natural Language Processing* - is shorthand for a wide array of techniques designed to help machines learn from text. Natural Language Processing powers everything from chatbots to search engines, and is used in diverse tasks like sentiment analysis and machine translation.\n",
    "\n",
    "In this tutorial we'll look at this competition's dataset, use a simple technique to process it, build a machine learning model, and submit predictions for a score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-03T13:03:45.396628Z",
     "iopub.status.busy": "2021-09-03T13:03:45.396334Z",
     "iopub.status.idle": "2021-09-03T13:03:47.816661Z",
     "shell.execute_reply": "2021-09-03T13:03:47.815834Z",
     "shell.execute_reply.started": "2021-09-03T13:03:45.396586Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-09-03T13:03:47.819117Z",
     "iopub.status.busy": "2021-09-03T13:03:47.818777Z",
     "iopub.status.idle": "2021-09-03T13:03:47.875433Z",
     "shell.execute_reply": "2021-09-03T13:03:47.874513Z",
     "shell.execute_reply.started": "2021-09-03T13:03:47.819063Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./train.csv\").fillna('')\n",
    "test_df = pd.read_csv(\"./test.csv\").fillna('')\n",
    "full_df = pd.concat([train_df, test_df]).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape, test_df.shape, full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Normalize spelling of twitter words, many informalities (include accented words and garbage characters)\n",
    "# Leave for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column, capturing tags\n",
    "# parse text for hashtags, and remove '#' symbol in the process\n",
    "import string\n",
    "\n",
    "\n",
    "def extract_tags(text):\n",
    "    unaccented_alnum = string.ascii_letters + ''.join(str(i) for i in range(10))\n",
    "    tags = []\n",
    "    for word in text.lower().split():\n",
    "        if word.startswith('#'):\n",
    "            tag = ''.join([t for t in word[1:] if t in unaccented_alnum])\n",
    "            tags.append(tag)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['tags'] = full_df['text'].apply(extract_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df.loc[full_df['id'] == 9652].text.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[full_df['id'] == 9652].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[full_df['id'] == 9652].tags.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:40:25.578501Z",
     "iopub.status.busy": "2021-09-03T13:40:25.578079Z",
     "iopub.status.idle": "2021-09-03T13:40:43.949731Z",
     "shell.execute_reply": "2021-09-03T13:40:43.948282Z",
     "shell.execute_reply.started": "2021-09-03T13:40:25.578462Z"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "# nlp.add_pipe(\"merge_noun_chunks\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-03T13:40:43.950972Z",
     "iopub.status.idle": "2021-09-03T13:40:43.951333Z"
    }
   },
   "outputs": [],
   "source": [
    "single_quote_unicode = ord(\"'\")\n",
    "translation_table_text = str.maketrans(\n",
    "    {\n",
    "        '`': single_quote_unicode,\n",
    "        '‘': single_quote_unicode,\n",
    "        '’': single_quote_unicode,\n",
    "        '“': single_quote_unicode,\n",
    "        '”': single_quote_unicode,\n",
    "        '-': None,\n",
    "    }\n",
    ")\n",
    "translation_table_token = str.maketrans(\n",
    "    {\n",
    "        \"'\": None,\n",
    "        '\"': None,\n",
    "        '.': None\n",
    "    }\n",
    ")\n",
    "\n",
    "def sub_token(token):\n",
    "    token_lowered = token.lower()\n",
    "    if 'http' in token_lowered:\n",
    "        return 'url'\n",
    "    elif '@' in token_lowered:\n",
    "        return 'usermention'\n",
    "    elif '&amp;' in token_lowered:\n",
    "        return 'and'\n",
    "    elif \"ain't\" in token_lowered:\n",
    "        return 'am not'\n",
    "    elif '\\x89û_' in token_lowered:\n",
    "        return f'{token_lowered[:-3]} ...'\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "def corpus2tokens(corpus_text, *args, **kwargs):\n",
    "    corpus_text = [' '.join([sub_token(token) for token in text.split()]) for text in corpus_text]\n",
    "    return [doc2tokens(doc) for doc in nlp.pipe(tqdm.notebook.tqdm(corpus_text), *args, **kwargs)]\n",
    "\n",
    "def doc2tokens(doc):\n",
    "    tokens = [token for token in doc if not (token.is_punct or token.is_space)]\n",
    "    return process_tokens(tokens, doc.ents)\n",
    "\n",
    "def show_ents(ents):\n",
    "    for ent in ents:\n",
    "        print(ent.text+' - ' +str(ent.start_char) +' - '+ str(ent.end_char) +' - '+ent.label_+ ' - '+str(spacy.explain(ent.label_)))\n",
    "\n",
    "def process_tokens(tokens, ents, rm_stopwords=False):\n",
    "    ent_vals_to_skip = ['#', '\\\\\\\\\\\\']\n",
    "    ent_labels_to_sub = [\n",
    "        \"DATE\", # Absolute or relative dates or periods\n",
    "        \"CARDINAL\", # Numerals that do not fall under another type\n",
    "        \"PERCENT\", # Percentage, including \"%\"\n",
    "        \"TIME\", # Times smaller than a day\n",
    "        \"MONEY\", # Monetary values, including unit\n",
    "        \"ORDINAL\", # \"first\", \"second\", etc.\n",
    "    ]\n",
    "    tokens_processed = []\n",
    "    stringed_ents = [ent.text.lower() for ent in ents if ent.text not in ent_vals_to_skip]\n",
    "#     print([(ent.text.lower(), ent.label_) for ent in ents])\n",
    "    ent_tokens = []\n",
    "    for token in tokens:\n",
    "        stringed_token = token.text.lower()\n",
    "        if stringed_token in stringed_ents:\n",
    "            ent_tokens.append(stringed_token)\n",
    "            ent_label = ents[stringed_ents.index(stringed_token)].label_\n",
    "            if ent_label in ent_labels_to_sub:\n",
    "                tokens_processed.append(ent_label)\n",
    "                continue\n",
    "#             stringed_token = ent_label + \"|\" + stringed_token.translate(translation_table_token)\n",
    "            stringed_token = stringed_token.translate(translation_table_token)\n",
    "        if rm_stopwords:\n",
    "            if stringed_token not in STOP_WORDS:\n",
    "                tokens_processed.append(stringed_token)\n",
    "        else:\n",
    "            tokens_processed.append(stringed_token)\n",
    "    len_ent_tokens = len(set(ent_tokens))\n",
    "    len_stringed_ents = len(set(stringed_ents))\n",
    "    if len_ent_tokens != len_stringed_ents:\n",
    "        print(f'WARNING: Somehow the number of unique tokens which are ents ({len_ent_tokens}) does not match the total number of unique ents ({len_stringed_ents})')\n",
    "        diff = list(set(stringed_ents) - set(ent_tokens))\n",
    "        if not diff:\n",
    "            diff = list(set(ent_tokens) - set(stringed_ents))\n",
    "            print(diff, \"exist in tokens but not in ents\")\n",
    "        print(diff, \"exist in ents but not in tokens\")\n",
    "        print(\"tokens: \", \"\\n\", tokens, \"\\n\\n\")\n",
    "        print(\"ents: \", \"\\n\", ents, \"\\n\\n\")\n",
    "    return tokens_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-03T13:40:43.952330Z",
     "iopub.status.idle": "2021-09-03T13:40:43.952722Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "corpus_text_full = [\n",
    "    item.translate(translation_table_text)\n",
    "    for item in full_df.text.to_list()\n",
    "]\n",
    "corpus_text_tokens_full = corpus2tokens(corpus_text_full)\n",
    "corpus_tags_full = full_df.tags.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def friendly_tag_corpus(row):\n",
    "    return [\n",
    "        f'id:{row.id}',\n",
    "        *[f'keyword:{k}' for k in [row.keyword] if k],\n",
    "        *[f'tag:{t}' for t in row.tags if t]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tags_friendly_full = full_df[[\"id\", \"keyword\", \"tags\"]].apply(friendly_tag_corpus, axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tag_id_mapping(corpus_tags):\n",
    "    tags = list(set(tag for tags in corpus_tags for tag in tags))\n",
    "    return {tag: idx for idx, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_id_mapping = build_tag_id_mapping(corpus_tags_friendly_full)\n",
    "id_tag_mapping = {v: k for k, v in tag_id_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tags_full = [[tag_id_mapping[tag] for tag in tags] for tags in corpus_tags_friendly_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Spacy confused #hashtags for MONEY often\n",
    "doc_idx = 3243\n",
    "print('ORIGINAL_TEXT:', corpus_text_full[doc_idx])\n",
    "print('TOKENS:', corpus_text_tokens_full[doc_idx])\n",
    "print('KEYWORD:', full_df.iloc[doc_idx].keyword)\n",
    "print('DOC2VEC TAG IDS:', corpus_tags_full[doc_idx])\n",
    "print('DOC2VEC TAG IDS:', corpus_tags_friendly_full[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = 21\n",
    "print('ORIGINAL_TEXT:', corpus_text_full[doc_idx])\n",
    "print('TOKENS:', corpus_text_tokens_full[doc_idx])\n",
    "print('KEYWORD:', full_df.iloc[doc_idx].keyword)\n",
    "print('DOC2VEC TAG IDS:', corpus_tags_full[doc_idx])\n",
    "print('DOC2VEC TAG IDS:', corpus_tags_friendly_full[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = 2936\n",
    "print('ORIGINAL_TEXT:', corpus_text_full[doc_idx])\n",
    "print('TOKENS:', corpus_text_tokens_full[doc_idx])\n",
    "print('KEYWORD:', full_df.iloc[doc_idx].keyword)\n",
    "print('DOC2VEC TAG IDS:', corpus_tags_full[doc_idx])\n",
    "print('DOC2VEC TAG IDS:', corpus_tags_friendly_full[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tagged_docs(corpus_words, corpus_tags):\n",
    "    return [TaggedDocument(doc_words, doc_tags) for doc_words, doc_tags in zip(corpus_words, corpus_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_full = gen_tagged_docs(corpus_text_tokens_full, corpus_tags_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_full[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://groups.google.com/g/gensim/c/6JmSsx4iIv0\n",
    "# projects with larger vocabularies tend to lean more towards negative-sampling than hierarchical-softmax\n",
    "# VERY NB - https://stackoverflow.com/a/37502976/1782641\n",
    "# https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "model = Doc2Vec(\n",
    "    epochs=1000,\n",
    "    workers=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.build_vocab(corpus_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Word 'airport' appeared {model.wv.get_vecattr('airport', 'count')} times in the full corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(corpus_full, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_similar_article_and_categories(corpus, doc_id=0, topn=10, by_article_tokens=True, by_article_tag=False):\n",
    "    doc = corpus[doc_id].words\n",
    "    print(' '.join(doc)[:200])\n",
    "\n",
    "    if by_article_tokens:\n",
    "        # Using words\n",
    "        print(\"************\")    \n",
    "        print(\"Get simlarity based on tokens:\")\n",
    "        print()    \n",
    "        inferred_vector = model.infer_vector(doc)\n",
    "        sims = model.dv.most_similar([inferred_vector], topn=topn)\n",
    "        for idx, factor in sims:\n",
    "            print(factor, id_tag_mapping[idx])  \n",
    "\n",
    "    if by_article_tag:\n",
    "        # Using doc vector\n",
    "        print(\"************\")    \n",
    "        print(\"Get simlarity based on article tag:\")\n",
    "        print()    \n",
    "        inferred_vector = model.dv[corpus[doc_id].tags[0]]\n",
    "        sims = model.dv.most_similar([inferred_vector], topn=topn)\n",
    "        for idx, factor in sims:\n",
    "            print(factor, id_tag_mapping[idx])\n",
    "    \n",
    "    print(\"************\")\n",
    "    print(\"Actual known tags:\")\n",
    "    print()\n",
    "    print([id_tag_mapping.get(tag) for tag in corpus[doc_id].tags if tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def rank_by_inferredvector(corpus, sent_ids):\n",
    "    ranks = []\n",
    "    for sent_id in sent_ids:\n",
    "        inferred_vector = model.infer_vector(corpus[sent_id].words)\n",
    "        sims = model.dv.most_similar([inferred_vector], topn=len(id_tag_mapping))\n",
    "        most_similar_tag_indices = [\n",
    "            [docid for docid, _ in sims].index(tag)\n",
    "            for tag in corpus[sent_id].tags if tag\n",
    "        ]\n",
    "        if most_similar_tag_indices:\n",
    "            rank = min(most_similar_tag_indices)\n",
    "            print(f'{sent_id}: Ranked {rank} ({id_tag_mapping[sims[rank][0]]}) out of {len(sims)}')\n",
    "            ranks.append(rank)\n",
    "    return ranks\n",
    "\n",
    "            \n",
    "def rank_by_random(corpus, sent_ids):\n",
    "    return [random.randint(0, len(id_tag_mapping)) for _ in sent_ids]\n",
    "\n",
    "\n",
    "def plot_matches(corpus, rank_func=rank_by_inferredvector, take_sample=True, sample_size=50, sample_seed=42, topn_perc=0.1):\n",
    "    if take_sample:\n",
    "        random.seed(sample_seed)\n",
    "        sent_ids = random.sample(range(0, len(corpus)), sample_size)\n",
    "    else:\n",
    "        sent_ids = list(range(len(corpus)))\n",
    "    ranks = rank_func(corpus, sent_ids)\n",
    "    counter = collections.Counter(ranks)\n",
    "    group_0 = []\n",
    "    group_1 = []\n",
    "    group_2 = []\n",
    "    for k, v in counter.items():\n",
    "        if k == 0:\n",
    "            group_0.append(v)\n",
    "        elif k < len(id_tag_mapping) / (100 / topn_perc):\n",
    "            group_1.append(v)\n",
    "        else:\n",
    "            group_2.append(v)\n",
    "        sum_0 = sum(group_0)\n",
    "        sum_1_acceptable = sum(group_1)\n",
    "        sum_all_else = sum(group_2)\n",
    "    plt.bar([0,1,2], [sum_0, sum_1_acceptable, sum_all_else])\n",
    "    print([sum_0, sum_1_acceptable, sum_all_else])\n",
    "    print('Test example correctly matched (%): ', 100 * sum_0 / sum([sum_0, sum_1_acceptable, sum_all_else]))\n",
    "    print(f'Test example matched in top {topn_perc}% (%): ', 100 * sum_1_acceptable / sum([sum_0, sum_1_acceptable, sum_all_else]))\n",
    "    print('Test example badly matched (%): ', 100 * sum_all_else / sum([sum_0, sum_1_acceptable, sum_all_else]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import simplejson\n",
    "\n",
    "# def json_load(filename):\n",
    "#     with open(filename, 'r', encoding='utf-8') as f:\n",
    "#         return simplejson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# def gen_tagged_docs_from_save(corpus):\n",
    "#     return [TaggedDocument(doc[\"words\"], doc[\"tags\"]) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.doc2vec import Doc2Vec\n",
    "# model = Doc2Vec.load('./doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# wv = KeyedVectors.load('./doc2vec.wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_train_full = json_load('./doc2vec.corpus.full.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_full = gen_tagged_docs_from_save(corpus_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_id_mapping = json_load('./doc2vec.tag_id_mapping.json')\n",
    "# id_tag_mapping = {v: k for k, v in tag_id_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_matches(corpus_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[full_df.id == 10873]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tennessee lesbian couple faked hate crime and destroyed own home with arson url lesbian\n",
    "# ************\n",
    "# Get simlarity based on tokens:\n",
    "\n",
    "# 0.6994612216949463 id:589\n",
    "# 0.6994302868843079 tag:lesbian\n",
    "# 0.6487046480178833 keyword:arson\n",
    "# 0.6095342040061951 id:10539\n",
    "# 0.6010091304779053 id:7573\n",
    "# 0.5964393019676208 id:6186\n",
    "# 0.5915811061859131 id:10873\n",
    "# 0.5876895189285278 id:8394\n",
    "# 0.5861586928367615 id:10562\n",
    "# 0.5838110446929932 id:10817\n",
    "# ************\n",
    "# Get simlarity based on article tag:\n",
    "\n",
    "# 0.9999999403953552 id:589\n",
    "# 0.999990701675415 tag:lesbian\n",
    "# 0.4069943130016327 id:8591\n",
    "# 0.40621358156204224 id:73\n",
    "# 0.40453746914863586 id:8779\n",
    "# 0.4021869897842407 id:9698\n",
    "# 0.4013387858867645 id:567\n",
    "# 0.3876795172691345 tag:weddinghour\n",
    "# 0.387553870677948 id:761\n",
    "# 0.3804241120815277 id:823\n",
    "# ************\n",
    "# Actual known tags:\n",
    "\n",
    "# ['id:589', 'keyword:arson', 'tag:lesbian']\n",
    "\n",
    "display_similar_article_and_categories(corpus_full, doc_id=409, by_article_tag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv\n",
    "wv.save('./doc2vec.wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_dicts(corpus):\n",
    "    for doc in corpus:\n",
    "        yield {\n",
    "            'words': doc.words,\n",
    "            'tags': doc.tags\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson\n",
    "\n",
    "\n",
    "def json_save(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        simplejson.dump(data, f, separators=(',', ':'), iterable_as_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_save(corpus_to_dicts(corpus_full), './doc2vec.corpus.full.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_save(tag_id_mapping, './doc2vec.tag_id_mapping.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick look at our data\n",
    "\n",
    "Let's look at our data... first, an example of what is NOT a disaster tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:03:48.835682Z",
     "iopub.status.busy": "2021-09-03T13:03:48.835383Z",
     "iopub.status.idle": "2021-09-03T13:03:48.853004Z",
     "shell.execute_reply": "2021-09-03T13:03:48.852258Z",
     "shell.execute_reply.started": "2021-09-03T13:03:48.835628Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[train_df[\"target\"] == 0][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one that is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:03:49.818411Z",
     "iopub.status.busy": "2021-09-03T13:03:49.817998Z",
     "iopub.status.idle": "2021-09-03T13:03:49.825759Z",
     "shell.execute_reply": "2021-09-03T13:03:49.824923Z",
     "shell.execute_reply.started": "2021-09-03T13:03:49.818371Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[train_df[\"target\"] == 1][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:06:04.089389Z",
     "iopub.status.busy": "2021-09-03T13:06:04.088820Z",
     "iopub.status.idle": "2021-09-03T13:06:04.361567Z",
     "shell.execute_reply": "2021-09-03T13:06:04.360871Z",
     "shell.execute_reply.started": "2021-09-03T13:06:04.089343Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vectors\n",
    "\n",
    "We have document vectors to use. We can infer vectors from any inputs (tokenized in the same way as in training the doc2vec model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text_tokens_train = corpus_text_tokens_full[:7613]\n",
    "corpus_text_tokens_test = corpus_text_tokens_full[7613:]\n",
    "len(corpus_text_tokens_train), len(corpus_text_tokens_test), len(corpus_text_tokens_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = [model.infer_vector(doc) for doc in corpus_text_tokens_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = [model.infer_vector(doc) for doc in corpus_text_tokens_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model\n",
    "\n",
    "As we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n",
    "\n",
    "What we're assuming here is a _linear_ connection. So let's build a linear model and see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, linear_model, ensemble, model_selection, preprocessing\n",
    "\n",
    "clf = ensemble.RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:05:21.781090Z",
     "iopub.status.busy": "2021-09-03T13:05:21.780806Z",
     "iopub.status.idle": "2021-09-03T13:05:22.292901Z",
     "shell.execute_reply": "2021-09-03T13:05:22.292141Z",
     "shell.execute_reply.started": "2021-09-03T13:05:21.781050Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:05:28.531714Z",
     "iopub.status.busy": "2021-09-03T13:05:28.531424Z",
     "iopub.status.idle": "2021-09-03T13:05:28.545507Z",
     "shell.execute_reply": "2021-09-03T13:05:28.544881Z",
     "shell.execute_reply.started": "2021-09-03T13:05:28.531673Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:05:31.729920Z",
     "iopub.status.busy": "2021-09-03T13:05:31.729467Z",
     "iopub.status.idle": "2021-09-03T13:05:31.734431Z",
     "shell.execute_reply": "2021-09-03T13:05:31.733861Z",
     "shell.execute_reply.started": "2021-09-03T13:05:31.729882Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission[\"target\"] = clf.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:05:34.012974Z",
     "iopub.status.busy": "2021-09-03T13:05:34.012566Z",
     "iopub.status.idle": "2021-09-03T13:05:34.027708Z",
     "shell.execute_reply": "2021-09-03T13:05:34.027079Z",
     "shell.execute_reply.started": "2021-09-03T13:05:34.012937Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:05:39.411323Z",
     "iopub.status.busy": "2021-09-03T13:05:39.410795Z",
     "iopub.status.idle": "2021-09-03T13:05:39.852139Z",
     "shell.execute_reply": "2021-09-03T13:05:39.851530Z",
     "shell.execute_reply.started": "2021-09-03T13:05:39.411278Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the viewer, you can submit the above file to the competition! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
