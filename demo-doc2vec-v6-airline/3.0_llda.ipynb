{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99cbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09078895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('./full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0198e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31117dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "# nlp.add_pipe(\"merge_noun_chunks\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERSON:      People, including fictional.\n",
    "# NORP:        Nationalities or religious or political groups.\n",
    "# FAC:         Buildings, airports, highways, bridges, etc.\n",
    "# ORG:         Companies, agencies, institutions, etc.\n",
    "# GPE:         Countries, cities, states.\n",
    "# LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "# PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "# EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "# WORK_OF_ART: Titles of books, songs, etc.\n",
    "# LAW:         Named documents made into laws.\n",
    "# LANGUAGE:    Any named language.\n",
    "# DATE:        Absolute or relative dates or periods.\n",
    "# TIME:        Times smaller than a day.\n",
    "# PERCENT:     Percentage, including ”%“.\n",
    "# MONEY:       Monetary values, including unit.\n",
    "# QUANTITY:    Measurements, as of weight or distance.\n",
    "# ORDINAL:     “first”, “second”, etc.\n",
    "# CARDINAL:    Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81239d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737bad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "single_quote_unicode = ord(\"'\")\n",
    "translation_table_text = str.maketrans(\n",
    "    {\n",
    "        '`': single_quote_unicode,\n",
    "        '‘': single_quote_unicode,\n",
    "        '’': single_quote_unicode,\n",
    "        '“': single_quote_unicode,\n",
    "        '”': single_quote_unicode,\n",
    "        '-': None,\n",
    "    }\n",
    ")\n",
    "translation_table_token = str.maketrans(\n",
    "    {\n",
    "        \"'\": None,\n",
    "        '\"': None,\n",
    "        '.': None\n",
    "    }\n",
    ")\n",
    "translation_table_title = str.maketrans(\n",
    "    {\n",
    "        '`': single_quote_unicode,\n",
    "        '‘': single_quote_unicode,\n",
    "        '’': single_quote_unicode,\n",
    "        '“': single_quote_unicode,\n",
    "        '”': single_quote_unicode,\n",
    "        ',': None,\n",
    "        '-': None,\n",
    "        '.': None        \n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "def titles2tokens(titles_text):\n",
    "    return [title2tokens(title_text) for title_text in tqdm_notebook(titles_text)]\n",
    "\n",
    "def title2tokens(title_text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(title_text)\n",
    "    return r.get_ranked_phrases()\n",
    "\n",
    "def corpus2tokens(corpus_text, *args, **kwargs):\n",
    "    return [doc2tokens(doc) for doc in nlp.pipe(tqdm_notebook(corpus_text), *args, **kwargs)]\n",
    "\n",
    "def doc2tokens(doc):\n",
    "    tokens = [token for token in doc if not (token.is_punct or token.is_space)]\n",
    "    return process_tokens(tokens, doc.ents)\n",
    "\n",
    "def show_ents(ents):\n",
    "    for ent in ents:\n",
    "        print(ent.text+' - ' +str(ent.start_char) +' - '+ str(ent.end_char) +' - '+ent.label_+ ' - '+str(spacy.explain(ent.label_)))\n",
    "        \n",
    "def process_tokens(tokens, ents):\n",
    "\n",
    "    \n",
    "    ent_labels_to_sub = [\n",
    "        \"DATE\", # Absolute or relative dates or periods\n",
    "        \"CARDINAL\", # Numerals that do not fall under another type\n",
    "        \"PERCENT\", # Percentage, including \"%\"\n",
    "        \"TIME\", # Times smaller than a day\n",
    "        \"MONEY\", # Monetary values, including unit\n",
    "        \"ORDINAL\", # \"first\", \"second\", etc.\n",
    "    ]\n",
    "    tokens_processed = []\n",
    "    stringed_ents = [ent.text.lower() for ent in ents]\n",
    "    ent_tokens = []\n",
    "    for token in tokens:\n",
    "        stringed_token = token.text.lower()\n",
    "        if stringed_token in stringed_ents:\n",
    "            ent_tokens.append(stringed_token)\n",
    "            ent_label = ents[stringed_ents.index(stringed_token)].label_\n",
    "            if ent_label in ent_labels_to_sub:\n",
    "                tokens_processed.append(ent_label)\n",
    "                continue\n",
    "#             stringed_token = ent_label + \"|\" + stringed_token.translate(translation_table_token)\n",
    "            stringed_token = stringed_token.translate(translation_table_token)\n",
    "        if stringed_token not in STOP_WORDS:\n",
    "            tokens_processed.append(stringed_token)\n",
    "    len_ent_tokens = len(set(ent_tokens))\n",
    "    len_stringed_ents = len(set(stringed_ents))\n",
    "    if len_ent_tokens != len_stringed_ents:\n",
    "        print(f'WARNING: Somehow the number of unique tokens which are ents ({len_ent_tokens}) does not match the total number of unique ents ({len_stringed_ents})')\n",
    "        diff = list(set(stringed_ents) - set(ent_tokens))\n",
    "        if not diff:\n",
    "            diff = list(set(ent_tokens) - set(stringed_ents))\n",
    "            print(diff, \"exist in tokens but not in ents\")\n",
    "        print(diff, \"exist in ents but not in tokens\")\n",
    "        print(\"tokens: \", \"\\n\", tokens, \"\\n\\n\")\n",
    "        print(\"ents: \", \"\\n\", ents, \"\\n\\n\")\n",
    "    return tokens_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870434e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "corpus_text_full = [\n",
    "    item.translate(translation_table_text)\n",
    "    for item in df_full.text.to_list()\n",
    "]\n",
    "corpus_text_tokens_full = corpus2tokens(corpus_text_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus_title_full = [\n",
    "    item.translate(translation_table_title)\n",
    "    for item in df_full.title.to_list()\n",
    "]\n",
    "corpus_title_tokens_full = titles2tokens(corpus_title_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re, numpy, random, string, json, pyLDAvis, operator\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration=50\n",
    "samplesize = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac674d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.001\n",
    "beta=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13975759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLDA:\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    def set_corpus(self, labelset, corpus, labels):\n",
    "        labelset.insert(0, \"common\")\n",
    "        self.labelmap = dict(zip(labelset, range(len(labelset))))        \n",
    "        self.K = len(self.labelmap)\n",
    "        self.vocas = []\n",
    "        self.vocas_id = dict()\n",
    "        self.labels = numpy.array([self.complement_label(label) for label in labels])\n",
    "\n",
    "        \n",
    "        self.docs = [[self.term_to_id(term) for term in doc] for doc in corpus]        \n",
    "        M = len(corpus)\n",
    "        V = len(self.vocas)\n",
    "\n",
    "        self.z_m_n = []\n",
    "        self.n_m_z = numpy.zeros((M, self.K), dtype=int)\n",
    "        self.n_z_t = numpy.zeros((self.K, V), dtype=int)\n",
    "        self.n_z = numpy.zeros(self.K, dtype=int)        \n",
    "        for m, doc, label in zip(range(M), self.docs, self.labels):\n",
    "            N_m = len(doc)\n",
    "            #z_n = [label[x] for x in numpy.random.randint(len(label), size=N_m)]\n",
    "            z_n = [numpy.random.multinomial(1, label / label.sum()).argmax() for x in range(N_m)]\n",
    "            self.z_m_n.append(z_n)\n",
    "            for t, z in zip(doc, z_n):\n",
    "                self.n_m_z[m, z] += 1\n",
    "                self.n_z_t[z, t] += 1\n",
    "                self.n_z[z] += 1        \n",
    "    def complement_label(self, label):\n",
    "        if not label: return numpy.ones(len(self.labelmap))\n",
    "        vec = numpy.zeros(len(self.labelmap))\n",
    "        vec[0] = 1.0\n",
    "        for x in label:            \n",
    "            vec[self.labelmap[x]] = 1.0\n",
    "        return vec\n",
    "    def term_to_id(self, term):\n",
    "        if term not in self.vocas_id:\n",
    "            voca_id = len(self.vocas)\n",
    "            self.vocas_id[term] = voca_id\n",
    "            self.vocas.append(term)\n",
    "        else:\n",
    "            voca_id = self.vocas_id[term]\n",
    "        return voca_id\n",
    "    def perplexity(self, docs=None):\n",
    "        if docs == None: docs = self.docs\n",
    "        phi = self.phi()\n",
    "        thetas = self.theta()\n",
    "\n",
    "        log_per = N = 0\n",
    "        for doc, theta in zip(docs, thetas):\n",
    "            for w in doc:\n",
    "                log_per -= numpy.log(numpy.inner(phi[:,w], theta))\n",
    "            N += len(doc)\n",
    "        return numpy.exp(log_per / N)\n",
    "    def phi(self):\n",
    "        V = len(self.vocas)        \n",
    "        return (self.n_z_t + self.beta) / (self.n_z[:, numpy.newaxis] + V * self.beta)\n",
    "\n",
    "    def theta(self):\n",
    "        \"\"\"document-topic distribution\"\"\"\n",
    "        n_alpha = self.n_m_z + self.labels * self.alpha\n",
    "        return n_alpha / n_alpha.sum(axis=1)[:, numpy.newaxis]\n",
    "    def inference(self):\n",
    "        V = len(self.vocas)\n",
    "        for m, doc, label in zip(range(len(self.docs)), self.docs, self.labels):\n",
    "            for n in range(len(doc)):\n",
    "                t = doc[n]\n",
    "                z = self.z_m_n[m][n]\n",
    "                self.n_m_z[m, z] -= 1\n",
    "                self.n_z_t[z, t] -= 1\n",
    "                self.n_z[z] -= 1\n",
    "\n",
    "                denom_a = self.n_m_z[m].sum() + self.K * self.alpha\n",
    "                denom_b = self.n_z_t.sum(axis=1) + V * self.beta\n",
    "                p_z = label * (self.n_z_t[:, t] + self.beta) / denom_b * (self.n_m_z[m] + self.alpha) / denom_a\n",
    "                new_z = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()\n",
    "\n",
    "                self.z_m_n[m][n] = new_z\n",
    "                self.n_m_z[m, new_z] += 1\n",
    "                self.n_z_t[new_z, t] += 1\n",
    "                self.n_z[new_z] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llda = LLDA(alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c67396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelset = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "# corpus = [\n",
    "#     [\"category\", \"Ay\", \"and\", \"Bee\"],\n",
    "#     [\"Not\", \"Ay\", \"Cee\", \"Dee\", \"Cee\"],\n",
    "#     [\"Give\", \"Me\", \"an\", \"Eee\"]\n",
    "\n",
    "# ]\n",
    "# labels = [\n",
    "#     [\"a\", \"b\"],\n",
    "#     [\"a\", \"c\", \"d\"],\n",
    "#     [\"e\"]\n",
    "# ]\n",
    "\n",
    "# llda.set_corpus(labelset, corpus, labels)\n",
    "labelset = list(set(reduce(list.__add__, corpus_title_tokens_full)))\n",
    "llda.set_corpus(labelset, corpus_text_tokens_full, corpus_title_tokens_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e999cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelmap = dict(zip(labelset, range(len(labelset))))            \n",
    "K = len(labelmap)\n",
    "print(\"M=%d, V=%d, L=%d, K=%d\" % (len(corpus), len(llda.vocas), len(labelset), K))\n",
    "print(\"len_corpus=%d, len_vocab=%d, len_labelset=%d, len_labelmap=%d\" % (len(corpus), len(llda.vocas), len(labelset), K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72408377",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(iteration):\n",
    "    perplexity = llda.perplexity()\n",
    "    sys.stderr.write(\"-- %d : %.4f\\n\" % (i, perplexity))\n",
    "    x.append(i)\n",
    "    y.append(perplexity)\n",
    "    llda.inference()\n",
    "print(\"perplexity : %.4f\" % llda.perplexity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8483050",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = llda.phi()\n",
    "result = {}\n",
    "for k, label in enumerate(labelset):\n",
    "    #print(\"\\n-- label %d : %s\" % (k, label))\n",
    "    result[label]=[]\n",
    "    for w in numpy.argsort(-phi[k])[:10]:\n",
    "        #print(\"%s: %.4f\" % (llda.vocas[w], phi[k,w]))\n",
    "        result[label].append(str(llda.vocas[w])+\":\"+str(phi[k,w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt \n",
    "\n",
    "#dat = np.random.randn(10,10)\n",
    "dat = phi\n",
    "#dat = np.array(dat)\n",
    "#dat = np.minimum(100, dat*100)\n",
    "#dat = dat.astype(np.int32)\n",
    "\n",
    "\n",
    "plt.imshow(dat, interpolation='none')\n",
    "\n",
    "clb = plt.colorbar()\n",
    "clb.ax.set_title('This is a title')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97cb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaResult = pd.DataFrame(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaResult.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594225f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = []\n",
    "for e in llda.docs:\n",
    "    doc_lengths.append(len(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc80db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = {}\n",
    "for doc in llda.docs:\n",
    "    for w in doc:\n",
    "        if w in tf:\n",
    "            tf[w]+=1\n",
    "        else:\n",
    "            tf[w]=1\n",
    "            \n",
    "tf2 = {}\n",
    "for e in llda.vocas_id:\n",
    "    tf2[e]=tf[llda.vocas_id[e]]\n",
    "    \n",
    "data = {'topic_term_dists': llda.phi(),\n",
    "        'doc_topic_dists': llda.theta(),\n",
    "        'vocab':llda.vocas,\n",
    "        'doc_lengths':doc_lengths,\n",
    "        'term_frequency':tf2\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_model_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Topic-Term shape: %s' % str(np.array(movies_model_data['topic_term_dists']).shape))\n",
    "print('Doc-Topic shape: %s' % str(np.array(movies_model_data['doc_topic_dists']).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a95b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.prepare(**movies_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x = sorted(llda.labelmap.items(), key=operator.itemgetter(1))\n",
    "pd.Series(sorted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ec344",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x = sorted(llda.labelmap.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29c5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llda.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da79e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ca1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_corpus(self, labelset, corpus, labels):\n",
    "#         labelset.insert(0, \"common\")\n",
    "#         self.labelmap = dict(zip(labelset, range(len(labelset))))        \n",
    "#         self.K = len(self.labelmap)\n",
    "#         self.vocas = []\n",
    "#         self.vocas_id = dict()\n",
    "        \n",
    "\n",
    "        \n",
    "#         self.docs = [[self.term_to_id(term) for term in doc] for doc in corpus]\n",
    "#         print(self.docs)\n",
    "#         M = len(corpus)\n",
    "#         V = len(self.vocas)\n",
    "\n",
    "#         self.z_m_n = []\n",
    "#         self.n_m_z = numpy.zeros((M, self.K), dtype=int)\n",
    "#         self.n_z_t = numpy.zeros((self.K, V), dtype=int)\n",
    "#         self.n_z = numpy.zeros(self.K, dtype=int)\n",
    "#         print(self.n_m_z)\n",
    "#         print(self.n_z_t)\n",
    "#         print(self.n_z)\n",
    "#         for m, doc, label in zip(range(M), self.docs, self.labels):\n",
    "#             N_m = len(doc)\n",
    "#             #z_n = [label[x] for x in numpy.random.randint(len(label), size=N_m)]\n",
    "#             z_n = [numpy.random.multinomial(1, label / label.sum()).argmax() for x in range(N_m)]\n",
    "#             self.z_m_n.append(z_n)\n",
    "#             for t, z in zip(doc, z_n):\n",
    "#                 self.n_m_z[m, z] += 1\n",
    "#                 self.n_z_t[z, t] += 1\n",
    "#                 self.n_z[z] += 1        \n",
    "#     def perplexity(self, docs=None):\n",
    "#         if docs == None: docs = self.docs\n",
    "#         phi = self.phi()\n",
    "#         thetas = self.theta()\n",
    "\n",
    "#         log_per = N = 0\n",
    "#         for doc, theta in zip(docs, thetas):\n",
    "#             for w in doc:\n",
    "#                 log_per -= numpy.log(numpy.inner(phi[:,w], theta))\n",
    "#             N += len(doc)\n",
    "#         return numpy.exp(log_per / N)\n",
    "#     def phi(self):\n",
    "        \n",
    "#         return (self.n_z_t + self.beta) / (self.n_z[:, numpy.newaxis] + V * self.beta)\n",
    "\n",
    "#     def inference(self):\n",
    "        \n",
    "#         for m, doc, label in zip(range(len(self.docs)), self.docs, self.labels):\n",
    "#             for n in range(len(doc)):\n",
    "#                 t = doc[n]\n",
    "#                 z = self.z_m_n[m][n]\n",
    "#                 self.n_m_z[m, z] -= 1\n",
    "#                 self.n_z_t[z, t] -= 1\n",
    "#                 self.n_z[z] -= 1\n",
    "\n",
    "#                 denom_a = self.n_m_z[m].sum() + self.K * self.alpha\n",
    "#                 denom_b = self.n_z_t.sum(axis=1) + V * self.beta\n",
    "#                 p_z = label * (self.n_z_t[:, t] + self.beta) / denom_b * (self.n_m_z[m] + self.alpha) / denom_a\n",
    "#                 new_z = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81212fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in (llda.vocas_id).items():\n",
    "    if v == 2:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llda.vocas_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llda.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af94abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llda.vocas_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [0, 1, 2]\n",
    "f, ax= plt.subplots(len(doc_ids), 1, figsize=(8, 12), sharex=True)\n",
    "K = len(labelmap)\n",
    "for i, k in enumerate(doc_ids):\n",
    "    #ax[i].stem(doc_topic[k,:], linefmt='r-',\n",
    "    ax[i].stem(llda.theta()[k], linefmt='r-',\n",
    "               markerfmt='ro', basefmt='w-')\n",
    "    ax[i].set_xlim(-1, K+1)\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(f\"Document {k}:\\n{' '.join(corpus[k])}\")\n",
    "    \n",
    "    ax[i].set_xlabel(\"Topic\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llda.theta()[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbe513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d1634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
