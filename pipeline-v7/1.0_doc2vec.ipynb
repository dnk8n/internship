{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed261e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_csv(filepath):\n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        return list(csv.DictReader(csvfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = load_csv('../data/BBairline200722_coreffed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ed7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_quote_unicode = ord(\"'\")\n",
    "translation_table_text = str.maketrans(\n",
    "    {\n",
    "        '`': single_quote_unicode,\n",
    "        '‘': single_quote_unicode,\n",
    "        '’': single_quote_unicode,\n",
    "        '“': single_quote_unicode,\n",
    "        '”': single_quote_unicode,\n",
    "    }\n",
    ")\n",
    "     \n",
    "corpus_texts_full, corpus_titles_full = [], []\n",
    "for row in csv_data:\n",
    "    text, title = row['text'].translate(translation_table_text), row['title']\n",
    "    corpus_texts_full.append(text)\n",
    "    corpus_titles_full.append(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "# nlp.add_pipe(\"merge_noun_chunks\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from typing import NamedTuple\n",
    "\n",
    "tag_id_map = {}\n",
    "\n",
    "def update_tag_id_map(keys):\n",
    "    for key in keys:\n",
    "        if tag_id_map.get(key) == None:\n",
    "            tag_id_map[key] = len(tag_id_map)\n",
    "\n",
    "\n",
    "class TokenData(NamedTuple):\n",
    "    text: str\n",
    "    lower_: str\n",
    "    lemma_: str\n",
    "    ent_type_: str\n",
    "    pos_: str\n",
    "    tag_: str\n",
    "    is_punct: bool\n",
    "    is_space: bool\n",
    "    is_stop: bool\n",
    "\n",
    "\n",
    "def corpus2tokens(corpus_text, *args, **kwargs):\n",
    "    nlp_pipe = nlp.pipe(tqdm.notebook.tqdm(corpus_text), *args, **kwargs)\n",
    "    return [doc2tokens(doc_id, doc) for doc_id, doc in enumerate(nlp_pipe)]\n",
    "\n",
    "\n",
    "def doc2tokens(doc_id, doc):\n",
    "    update_tag_id_map((doc_id,))\n",
    "    return {\n",
    "        'doc_id': doc_id,\n",
    "        'tokens': [\n",
    "            TokenData(\n",
    "                text=token.text,\n",
    "                lower_=token.lower_,\n",
    "                lemma_=token.lemma_,\n",
    "                ent_type_=token.ent_type_,\n",
    "                pos_=token.pos_,\n",
    "                tag_=token.tag_,\n",
    "                is_punct=token.is_punct,\n",
    "                is_space=token.is_space,\n",
    "                is_stop=token.is_stop,\n",
    "            )\n",
    "            for token in doc\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# def doc2tokens(doc_id, doc):\n",
    "#     return [sent2tokens(doc_id, sent_id, sent) for sent_id, sent in enumerate(doc.sents)]\n",
    "\n",
    "\n",
    "# def sent2tokens(doc_id, sent_id, sent):\n",
    "#     compound_sent_id = (doc_id, sent_id,)\n",
    "#     update_tag_id_map((doc_id, compound_sent_id,))\n",
    "#     return {\n",
    "#         'sent_id': compound_sent_id,\n",
    "#         'tokens': [\n",
    "#             TokenData(\n",
    "#                 text=token.text,\n",
    "#                 lower_=token.lower_,\n",
    "#                 lemma_=token.lemma_,\n",
    "#                 ent_type_=token.ent_type_,\n",
    "#                 pos_=token.pos_,\n",
    "#                 tag_=token.tag_,\n",
    "#                 is_punct=token.is_punct,\n",
    "#                 is_space=token.is_space,\n",
    "#             )\n",
    "#             for token in sent\n",
    "#         ]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus_token_objects = corpus2tokens(corpus_texts_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69483f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_token_objects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in corpus_token_objects[0]['tokens']:\n",
    "    print((i.text, i.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# def gen_tagged_docs_by_sent(token_objects, process_tokens_func, process_tags_func):\n",
    "#     tagged_docs = []A\n",
    "#     for doc in token_objects:\n",
    "#         for sent in doc:\n",
    "#             sent_tokens = process_tokens_func(sent)\n",
    "#             sent_tags = process_tags_func(sent)\n",
    "#             tagged_docs.append(TaggedDocument(sent_tokens, sent_tags))\n",
    "#     return tagged_docs\n",
    "\n",
    "def gen_tagged_docs_by_doc(token_objects, process_tokens_func, process_tags_func):\n",
    "    tagged_docs = []\n",
    "    for doc in token_objects:\n",
    "        doc_tokens = process_tokens_func(doc)\n",
    "        doc_tags = process_tags_func(doc)\n",
    "        tagged_docs.append(TaggedDocument(doc_tokens, doc_tags))\n",
    "    return tagged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_tokens_sent(sent):\n",
    "#     return [token.lower_ for token in sent['tokens'] if not (token.is_punct or token.is_space)]\n",
    "\n",
    "# def process_tags_sent(sent):\n",
    "#     doc_id, sent_id = sent['sent_id']\n",
    "#     return [tag_id_map[doc_id], tag_id_map[doc_id, sent_id]]\n",
    "\n",
    "def process_tokens_doc(doc):\n",
    "    return [token.lower_ for token in doc['tokens'] if not (token.is_punct or token.is_space)]\n",
    "\n",
    "def process_tags_doc(doc):\n",
    "    return [tag_id_map[doc['doc_id']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_full = gen_tagged_docs_by_sent(corpus_token_objects, process_tokens_sent, process_tags_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_full = gen_tagged_docs_by_doc(corpus_token_objects, process_tokens_doc, process_tags_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaeb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_full[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://groups.google.com/g/gensim/c/6JmSsx4iIv0\n",
    "# projects with larger vocabularies tend to lean more towards negative-sampling than hierarchical-softmax\n",
    "# VERY NB - https://stackoverflow.com/a/37502976/1782641\n",
    "# https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "model = Doc2Vec(\n",
    "    vector_size=300,\n",
    "    epochs=200,\n",
    "    min_count=10,\n",
    "    window=10,\n",
    "    hs=0,\n",
    "    negative=20,\n",
    "    sample=1e-3,\n",
    "    workers=3  # 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.build_vocab(corpus_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeba1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Word 'airport' appeared {model.wv.get_vecattr('airport', 'count')} times in the full corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a1002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(corpus_full, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b29631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv\n",
    "wv.save('./doc2vec.wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_to_dicts(corpus):\n",
    "    for doc in corpus:\n",
    "        yield {\n",
    "            'tokens': doc.words,\n",
    "            'tags': doc.tags\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c674b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson\n",
    "\n",
    "\n",
    "def json_save(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        simplejson.dump(data, f, separators=(',', ':'), iterable_as_array=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_save(corpus_to_dicts(corpus_full), './doc2vec.corpus.full.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tag_map = tag_id_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa82abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_save(id_tag_map, './doc2vec.id_tag_map.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62bd482",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_save(corpus_token_objects, './doc2vec.corpus_token_objects.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742d9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
